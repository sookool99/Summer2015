{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Theano Tutorial\n",
    "\n",
    "Theano is a software package which allows you to write symbolic code and compile it onto different architectures (in particular, CPU and GPU).  It was developed by machine learning researchers at the University of Montreal.  Its use is not limited to machine learning applications, but it was designed with machine learning in mind.  It's especially good for machine learning techniques which are CPU-intensive and benefit from parallelization (e.g. large neural networks).\n",
    "\n",
    "This tutorial will cover the basic principles of Theano, including some common mental blocks which come up.  It will also cover a simple multi-layer perceptron example.  A more thorough Theano tutorial can be found here: http://deeplearning.net/software/theano/tutorial/\n",
    "\n",
    "Any comments or suggestions should be directed to [me](http://colinraffel.com/ \"Colin Raffel\") or feel free to [submit a pull request](https://github.com/craffel/theano-tutorial/pulls \"Pull Request\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e27d371d6baa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2228\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2229\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m     87\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_argstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3087\u001b[0m         \"\"\"\n\u001b[0;32m   3088\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3089\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[1;34m(gui, gui_select)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (No module named 'scipy')\n",
      "WARNING:theano.tensor.blas:Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (No module named 'scipy')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import theano\n",
    "# By convention, the tensor submodule is loaded as T\n",
    "import theano.tensor as T\n",
    "# from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Symbolic variables\n",
    "\n",
    "In Theano, all algorithms are defined symbolically.  It's more like writing out math than writing code.  The following Theano variables are symbolic; they don't have an explicit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "TensorType(float64, scalar)\n",
      "(foo ** TensorConstant{2})\n"
     ]
    }
   ],
   "source": [
    "# The theano.tensor submodule has various primitive symbolic variable types.\n",
    "# Here, we're defining a scalar (0-d) variable.\n",
    "# The argument gives the variable its name.\n",
    "# from __future__ import print_function\n",
    "foo = T.scalar('foo')\n",
    "# Now, we can define another variable bar which is just foo squared.\n",
    "bar = foo**2\n",
    "# It will also be a theano variable.\n",
    "print (type(bar))\n",
    "print (bar.type)\n",
    "# Using theano's pp (pretty print) function, we see that \n",
    "# bar is defined symbolically as the square of foo\n",
    "print (theano.pp(bar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "To actually compute things with Theano, you define symbolic functions, which can then be called with actual values to retrieve an actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# We can't compute anything with foo and bar yet.\n",
    "# We need to define a theano function first.\n",
    "# The first argument of theano.function defines the inputs to the function.\n",
    "# Note that bar relies on foo, so foo is an input to this function.\n",
    "# theano.function will compile code for computing values of bar given values of foo\n",
    "f = theano.function([foo], bar)\n",
    "print (f(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, in some cases you can use a symbolic variable's eval method.\n",
    "# This can be more convenient than defining a function.\n",
    "# The eval method takes a dictionary where the keys are theano variables and the values are values for those variables.\n",
    "print (bar.eval({foo: 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# We can also use Python functions to construct Theano variables.\n",
    "# It seems pedantic here, but can make syntax cleaner for more complicated examples.\n",
    "def square(x):\n",
    "    return x**2\n",
    "bar = square(foo)\n",
    "print (bar.eval({foo: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theano.tensor\n",
    "\n",
    "Theano also has variable types for vectors, matrices, and tensors.  The `theano.tensor` submodule has various functions for performing operations on these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 18.,  37.]), array(91.0)]\n",
      "[array([ 14.,  32.]), array(91.0)]\n"
     ]
    }
   ],
   "source": [
    "A = T.matrix('A')\n",
    "x = T.vector('x')\n",
    "b = T.vector('b')\n",
    "y = T.dot(A, x) + b\n",
    "# Note that squaring a matrix is element-wise\n",
    "z = T.sum(A**2)\n",
    "# theano.function can compute multiple things at a time\n",
    "# You can also set default parameter values\n",
    "# We'll cover theano.config.floatX later\n",
    "b_default = np.array([0, 0], dtype=theano.config.floatX)\n",
    "linear_mix = theano.function([A, x, theano.Param(b, default=b_default)], [y, z])\n",
    "# Supplying values for A, x, and b\n",
    "print ( linear_mix(np.array([[1, 2, 3],\n",
    "                           [4, 5, 6]], dtype=theano.config.floatX), #A\n",
    "                 np.array([1, 2, 3], dtype=theano.config.floatX), #x\n",
    "                 np.array([4, 5], dtype=theano.config.floatX)) ) #b\n",
    "# Using the default value for b\n",
    "print ( linear_mix(np.array([[1, 2, 3],\n",
    "                           [4, 5, 6]]), #A\n",
    "                 np.array([1, 2, 3])) ) #x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables\n",
    "\n",
    "Shared variables are a little different - they actually do have an explicit value, which can be get/set and is shared across functions which use the variable.  They're also useful because they have state across function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float64, matrix)>\n"
     ]
    }
   ],
   "source": [
    "shared_var = theano.shared(np.array([[1, 2], [3, 4]], dtype=theano.config.floatX))\n",
    "# The type of the shared variable is deduced from its initialization\n",
    "print (shared_var.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  4.]\n",
      " [ 2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# We can set the value of a shared variable using set_value\n",
    "shared_var.set_value(np.array([[3, 4], [2, 1]], dtype=theano.config.floatX))\n",
    "# ..and get it using get_value\n",
    "print (shared_var.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 4.  9.]]\n"
     ]
    }
   ],
   "source": [
    "shared_squared = shared_var**2\n",
    "# The first argument of theano.function (inputs) tells Theano what the arguments to the compiled function should be.\n",
    "# Note that because shared_var is shared, it already has a value, so it doesn't need to be an input to the function.\n",
    "# Therefore, Theano implicitly considers shared_var an input to a function using shared_squared and so we don't need\n",
    "# to include it in the inputs argument of theano.function.\n",
    "function_1 = theano.function([], shared_squared)\n",
    "print (function_1())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updates\n",
    "\n",
    "The value of a shared variable can be updated in a function by using the updates argument of `theano.function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_var before subtracting [[1, 1], [1, 1]] using function_2:\n",
      "[[ 1.  2.]\n",
      " [ 3.  4.]]\n",
      "shared_var after calling function_2:\n",
      "[[ 0.  1.]\n",
      " [ 2.  3.]]\n",
      "New output of function_1() (shared_var**2):\n",
      "[[ 0.  1.]\n",
      " [ 4.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# We can also update the state of a shared var in a function\n",
    "subtract = T.matrix('subtract')\n",
    "# updates takes a dict where keys are shared variables and values are the new value the shared variable should take\n",
    "# Here, updates will set shared_var = shared_var - subtract\n",
    "function_2 = theano.function([subtract], shared_var, updates={shared_var: shared_var - subtract})\n",
    "print (\"shared_var before subtracting [[1, 1], [1, 1]] using function_2:\")\n",
    "print (shared_var.get_value())\n",
    "# Subtract [[1, 1], [1, 1]] from shared_var\n",
    "function_2(np.array([[1, 1], [1, 1]]))\n",
    "print (\"shared_var after calling function_2:\")\n",
    "print (shared_var.get_value())\n",
    "# Note that this also changes the output of function_1, because shared_var is shared!\n",
    "print (\"New output of function_1() (shared_var**2):\")\n",
    "print (function_1())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "A pretty huge benefit of using Theano is its ability to compute gradients.  This allows you to symbolically define a function and quickly compute its (numerical) derivative without actually deriving the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(20.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall that bar = foo**2\n",
    "# We can compute the gradient of bar with respect to foo like so:\n",
    "bar_grad = T.grad(bar, foo)\n",
    "# We expect that bar_grad = 2*foo\n",
    "bar_grad.eval({foo: 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-62f55349f6c1>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-62f55349f6c1>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print linear_mix_J(np.array([[9, 8, 7], [4, 5, 6]]), #A\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Recall that y = Ax + b\n",
    "# We can also compute a Jacobian like so:\n",
    "y_J = theano.gradient.jacobian(y, x)\n",
    "linear_mix_J = theano.function([A, x, b], y_J)\n",
    "# Because it's a linear mix, we expect the output to always be A\n",
    "print ( linear_mix_J(np.array([[9, 8, 7], [4, 5, 6]]), #A\n",
    "                   np.array([1, 2, 3]), #x\n",
    "                   np.array([4, 5])) )#b\n",
    "# We can also compute the Hessian with theano.gradient.hessian (skipping that here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Mode\n",
    "\n",
    "Debugging in Theano can be a little tough because the code which is actually being run is pretty far removed from the code you wrote.  By default, Theano compiles your code to be as fast as possible.  However, you can compile it to allow for debugging, at the cost of speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# A simple division function\n",
    "num = T.scalar('num')\n",
    "den = T.scalar('den')\n",
    "divide = theano.function([num, den], num/den)\n",
    "print (divide(10, 2))\n",
    "# This will cause a NaN\n",
    "print (divide(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-359a7801c570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# To compile a function in debug mode, just set mode='DebugMode'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdivide\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mden\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DebugMode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# NaNs now cause errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num' is not defined"
     ]
    }
   ],
   "source": [
    "# To compile a function in debug mode, just set mode='DebugMode'\n",
    "divide = theano.function([num, den], num/den, mode='DebugMode')\n",
    "# NaNs now cause errors\n",
    "print (divide(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CPU vs GPU\n",
    "\n",
    "Theano can transparently compile onto different hardware.  What device it uses by default depends on your `.theanorc` file and any environment variables defined, as described in detail here: http://deeplearning.net/software/theano/library/config.html\n",
    "Currently, you should use float32 when using most GPUs, but most people prefer to use float64 on a CPU.  For convenience, Theano provides the floatX configuration variable which designates what float accuracy to use.  For example, you can run a Python script with certain environment variables set to use the CPU:\n",
    "\n",
    "`THEANO_FLAGS=device=cpu,floatX=float64 python your_script.py`\n",
    "\n",
    "or GPU:\n",
    "\n",
    "`THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# You can get the values being used to configure Theano like so:\n",
    "print (theano.config.device)\n",
    "print (theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also get/set them at runtime:\n",
    "old_floatX = theano.config.floatX\n",
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorType(float64, vector)>\n",
      "<TensorType(float32, vector)>\n"
     ]
    }
   ],
   "source": [
    "# Be careful that you're actually using floatX!\n",
    "# For example, the following will cause var to be a float64 regardless of floatX due to numpy defaults:\n",
    "var = theano.shared(np.array([1.3, 2.4]))\n",
    "print (var.type()) #!!!\n",
    "# So, whenever you use a numpy array, make sure to set its dtype to theano.config.floatX\n",
    "var = theano.shared(np.array([1.3, 2.4], dtype=theano.config.floatX))\n",
    "print (var.type())\n",
    "# Revert to old value\n",
    "theano.config.floatX = old_floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MLP\n",
    "\n",
    "Defining a multilayer perceptron is out of the scope of this tutorial; please see here for background information:\n",
    "http://en.wikipedia.org/wiki/Multilayer_perceptron.  We will be using the convention that datapoints are column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer class\n",
    "\n",
    "We'll be defining our multilayer perceptron as a series of \"layers\", each applied successively to the input to produce the network output.  Each layer is defined as a class, which stores a weight matrix and a bias vector and includes a function for computing the layer's output.  \n",
    "\n",
    "Note that if we weren't using Theano, we might expect the `output` method to take in a vector and return the layer's activation in response to this input.  However, with Theano, the `output` function is instead meant to be used to _create_ (using `theano.function`) a function which can take in a vector and return the layer's activation.  So, if you were to pass, say, a `np.ndarray` to the `Layer` class's `output` function, you'd get an error.  Instead, we'll construct a function for actually computing the `Layer`'s activation outside of the class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, W_init, b_init, activation):\n",
    "        '''\n",
    "        A layer of a neural network, computes s(Wx + b) where s is a nonlinearity and x is the input vector.\n",
    "\n",
    "        :parameters:\n",
    "            - W_init : np.ndarray, shape=(n_output, n_input)\n",
    "                Values to initialize the weight matrix to.\n",
    "            - b_init : np.ndarray, shape=(n_output,)\n",
    "                Values to initialize the bias vector\n",
    "            - activation : theano.tensor.elemwise.Elemwise\n",
    "                Activation function for layer output\n",
    "        '''\n",
    "        # Retrieve the input and output dimensionality based on W's initialization\n",
    "        n_output, n_input = W_init.shape\n",
    "        # Make sure b is n_output in size\n",
    "        assert b_init.shape == (n_output,)\n",
    "        # All parameters should be shared variables.\n",
    "        # They're used in this class to compute the layer output,\n",
    "        # but are updated elsewhere when optimizing the network parameters.\n",
    "        # Note that we are explicitly requiring that W_init has the theano.config.floatX dtype\n",
    "        self.W = theano.shared(value=W_init.astype(theano.config.floatX),\n",
    "                               # The name parameter is solely for printing purporses\n",
    "                               name='W',\n",
    "                               # Setting borrow=True allows Theano to use user memory for this object.\n",
    "                               # It can make code slightly faster by avoiding a deep copy on construction.\n",
    "                               # For more details, see\n",
    "                               # http://deeplearning.net/software/theano/tutorial/aliasing.html\n",
    "                               borrow=True)\n",
    "        # We can force our bias vector b to be a column vector using numpy's reshape method.\n",
    "        # When b is a column vector, we can pass a matrix-shaped input to the layer\n",
    "        # and get a matrix-shaped output, thanks to broadcasting (described below)\n",
    "        self.b = theano.shared(value=b_init.reshape(-1, 1).astype(theano.config.floatX),\n",
    "                               name='b',\n",
    "                               borrow=True,\n",
    "                               # Theano allows for broadcasting, similar to numpy.\n",
    "                               # However, you need to explicitly denote which axes can be broadcasted.\n",
    "                               # By setting broadcastable=(False, True), we are denoting that b\n",
    "                               # can be broadcast (copied) along its second dimension in order to be\n",
    "                               # added to another variable.  For more information, see\n",
    "                               # http://deeplearning.net/software/theano/library/tensor/basic.html\n",
    "                               broadcastable=(False, True))\n",
    "        self.activation = activation\n",
    "        # We'll compute the gradient of the cost of the network with respect to the parameters in this list.\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def output(self, x):\n",
    "        '''\n",
    "        Compute this layer's output given an input\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for layer input\n",
    "\n",
    "        :returns:\n",
    "            - output : theano.tensor.var.TensorVariable\n",
    "                Mixed, biased, and activated x\n",
    "        '''\n",
    "        # Compute linear mix\n",
    "        lin_output = T.dot(self.W, x) + self.b\n",
    "        # Output is just linear mix if no activation function\n",
    "        # Otherwise, apply the activation function\n",
    "        return (lin_output if self.activation is None else self.activation(lin_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP class\n",
    "\n",
    "Most of the functionality of our MLP is contained in the `Layer` class; the `MLP` class is essentially just a container for a list of `Layer`s and their parameters.  The `output` function simply recursively computes the output for each layer.  Finally, the `squared_error` returns the squared Euclidean distance between the output of the network given an input and the desired (ground truth) output.  This function is meant to be used as a cost in the setting of minimizing cost over some training data.  As above, the `output` and `squared error` functions are not to be used for actually computing values; instead, they're to be used to create functions which are used to compute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, W_init, b_init, activations):\n",
    "        '''\n",
    "        Multi-layer perceptron class, computes the composition of a sequence of Layers\n",
    "\n",
    "        :parameters:\n",
    "            - W_init : list of np.ndarray, len=N\n",
    "                Values to initialize the weight matrix in each layer to.\n",
    "                The layer sizes will be inferred from the shape of each matrix in W_init\n",
    "            - b_init : list of np.ndarray, len=N\n",
    "                Values to initialize the bias vector in each layer to\n",
    "            - activations : list of theano.tensor.elemwise.Elemwise, len=N\n",
    "                Activation function for layer output for each layer\n",
    "        '''\n",
    "        # Make sure the input lists are all of the same length\n",
    "        assert len(W_init) == len(b_init) == len(activations)\n",
    "        \n",
    "        # Initialize lists of layers\n",
    "        self.layers = []\n",
    "        # Construct the layers\n",
    "        for W, b, activation in zip(W_init, b_init, activations):\n",
    "            self.layers.append(Layer(W, b, activation))\n",
    "\n",
    "        # Combine parameters from all layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "        \n",
    "    def output(self, x):\n",
    "        '''\n",
    "        Compute the MLP's output given an input\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for network input\n",
    "\n",
    "        :returns:\n",
    "            - output : theano.tensor.var.TensorVariable\n",
    "                x passed through the MLP\n",
    "        '''\n",
    "        # Recursively compute output\n",
    "        for layer in self.layers:\n",
    "            x = layer.output(x)\n",
    "        return x\n",
    "\n",
    "    def squared_error(self, x, y):\n",
    "        '''\n",
    "        Compute the squared euclidean error of the network output against the \"true\" output y\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for network input\n",
    "            - y : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for desired network output\n",
    "\n",
    "        :returns:\n",
    "            - error : theano.tensor.var.TensorVariable\n",
    "                The squared Euclidian distance between the network output and y\n",
    "        '''\n",
    "        return T.sum((self.output(x) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "To train the network, we will minimize the cost (squared Euclidean distance of network output vs. ground-truth) over a training set using gradient descent.  When doing gradient descent on neural nets, it's very common to use momentum, which is simply a leaky integrator on the parameter update.  That is, when updating parameters, a linear mix of the current gradient update and the previous gradient update is computed.  This tends to make the network converge more quickly on a good solution and can help avoid local minima in the cost function.  With traditional gradient descent, we are guaranteed to decrease the cost at each iteration.  When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "In Theano, we store the previous parameter update as a shared variable so that its value is preserved across iterations.  Then, during the gradient update, we not only update the parameters, but we also update the previous parameter update shared variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_updates_momentum(cost, params, learning_rate, momentum):\n",
    "    '''\n",
    "    Compute updates for gradient descent with momentum\n",
    "    \n",
    "    :parameters:\n",
    "        - cost : theano.tensor.var.TensorVariable\n",
    "            Theano cost function to minimize\n",
    "        - params : list of theano.tensor.var.TensorVariable\n",
    "            Parameters to compute gradient against\n",
    "        - learning_rate : float\n",
    "            Gradient descent learning rate\n",
    "        - momentum : float\n",
    "            Momentum parameter, should be at least 0 (standard gradient descent) and less than 1\n",
    "   \n",
    "    :returns:\n",
    "        updates : list\n",
    "            List of updates, one for each parameter\n",
    "    '''\n",
    "    # Make sure momentum is a sane value\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    # List of update steps for each parameter\n",
    "    updates = []\n",
    "    # Just gradient descent on cost\n",
    "    for param in params:\n",
    "        # For each parameter, we'll create a param_update shared variable.\n",
    "        # This variable will keep track of the parameter's update step across iterations.\n",
    "        # We initialize it to 0\n",
    "        param_update = theano.shared(param.get_value()*0., broadcastable=param.broadcastable)\n",
    "        # Each parameter is updated by taking a step in the direction of the gradient.\n",
    "        # However, we also \"mix in\" the previous step according to the given momentum value.\n",
    "        # Note that when updating param_update, we are using its old value and also the new gradient step.\n",
    "        updates.append((param, param - learning_rate*param_update))\n",
    "        # Note that we don't need to derive backpropagation to compute updates - just use T.grad!\n",
    "        updates.append((param_update, momentum*param_update + (1. - momentum)*T.grad(cost, param)))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "We'll train our neural network to classify two Gaussian-distributed clusters in 2d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-92bf2a3359bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                np.random.randn(N)*covariances[1, y] + means[1, y]])\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Plot the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Training data - two randomly-generated Gaussian-distributed clouds of points in 2d space\n",
    "np.random.seed(0)\n",
    "# Number of points\n",
    "N = 1000\n",
    "# Labels for each cluster\n",
    "y = np.random.random_integers(0, 1, N)\n",
    "# Mean of each cluster\n",
    "means = np.array([[-1, 1], [-1, 1]])\n",
    "# Covariance (in X and Y direction) of each cluster\n",
    "covariances = np.random.random_sample((2, 2)) + 1\n",
    "# Dimensions of each point\n",
    "X = np.vstack([np.random.randn(N)*covariances[0, y] + means[0, y],\n",
    "               np.random.randn(N)*covariances[1, y] + means[1, y]])\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[0, :], X[1, :], c=y, lw=.3, s=3, cmap=plt.cm.cool)\n",
    "plt.axis([-6, 6, -6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, set the size of each layer (and the number of layers)\n",
    "# Input layer size is training data dimensionality (2)\n",
    "# Output size is just 1-d: class label - 0 or 1\n",
    "# Finally, let the hidden layers be twice the size of the input.\n",
    "# If we wanted more layers, we could just add another layer size to this list.\n",
    "layer_sizes = [X.shape[0], X.shape[0]*2, 1]\n",
    "# Set initial parameter values\n",
    "W_init = []\n",
    "b_init = []\n",
    "activations = []\n",
    "for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "    # Getting the correct initialization matters a lot for non-toy problems.\n",
    "    # However, here we can just use the following initialization with success:\n",
    "    # Normally distribute initial weights\n",
    "    W_init.append(np.random.randn(n_output, n_input))\n",
    "    # Set initial biases to 1\n",
    "    b_init.append(np.ones(n_output))\n",
    "    # We'll use sigmoid activation for all layers\n",
    "    # Note that this doesn't make a ton of sense when using squared distance\n",
    "    # because the sigmoid function is bounded on [0, 1].\n",
    "    activations.append(T.nnet.sigmoid)\n",
    "# Create an instance of the MLP class\n",
    "mlp = MLP(W_init, b_init, activations)\n",
    "\n",
    "# Create Theano variables for the MLP input\n",
    "mlp_input = T.matrix('mlp_input')\n",
    "# ... and the desired output\n",
    "mlp_target = T.vector('mlp_target')\n",
    "# Learning rate and momentum hyperparameter values\n",
    "# Again, for non-toy problems these values can make a big difference\n",
    "# as to whether the network (quickly) converges on a good local minimum.\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# Create a function for computing the cost of the network given an input\n",
    "cost = mlp.squared_error(mlp_input, mlp_target)\n",
    "# Create a theano function for training the network\n",
    "train = theano.function([mlp_input, mlp_target], cost,\n",
    "                        updates=gradient_updates_momentum(cost, mlp.params, learning_rate, momentum))\n",
    "# Create a theano function for computing the MLP's output given some input\n",
    "mlp_output = theano.function([mlp_input], mlp.output(mlp_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-add895eff852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_output\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Plot network output after this iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     plt.scatter(X[0, :], X[1, :], c=current_output,\n\u001b[0;32m     22\u001b[0m                 lw=.3, s=3, cmap=plt.cm.cool, vmin=0, vmax=1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Keep track of the number of training iterations performed\n",
    "iteration = 0\n",
    "# We'll only train the network with 20 iterations.\n",
    "# A more common technique is to use a hold-out validation set.\n",
    "# When the validation error starts to increase, the network is overfitting,\n",
    "# so we stop training the net.  This is called \"early stopping\", which we won't do here.\n",
    "max_iteration = 20\n",
    "while iteration < max_iteration:\n",
    "    # Train the network using the entire training set.\n",
    "    # With large datasets, it's much more common to use stochastic or mini-batch gradient descent\n",
    "    # where only a subset (or a single point) of the training set is used at each iteration.\n",
    "    # This can also help the network to avoid local minima.\n",
    "    current_cost = train(X, y)\n",
    "    # Get the current network output for all points in the training set\n",
    "    current_output = mlp_output(X)\n",
    "    # We can compute the accuracy by thresholding the output\n",
    "    # and computing the proportion of points whose class match the ground truth class.\n",
    "    accuracy = np.mean((current_output > .5) == y)\n",
    "    # Plot network output after this iteration\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(X[0, :], X[1, :], c=current_output,\n",
    "                lw=.3, s=3, cmap=plt.cm.cool, vmin=0, vmax=1)\n",
    "    plt.axis([-6, 6, -6, 6])\n",
    "    plt.title('Cost: {:.3f}, Accuracy: {:.3f}'.format(float(current_cost), accuracy))\n",
    "    plt.show()\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
